{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline training of an end-to-end clip\n",
    "# Encoder/decoder.\n",
    "# Try to return to torch_emb=False\n",
    "import torch.multiprocessing as mp\n",
    "from coati.training.train_coati import train_autoencoder, do_args\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "from coati.data.dataset import COATI_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = do_args()\n",
    "    args.nodes = 1  # total num nodes.\n",
    "    args.nr = 0  # rank of this node.\n",
    "    # note args.gpus will default to the # gpus on this node.\n",
    "    args.data_parallel = True\n",
    "\n",
    "    args.test_frac = 0.02\n",
    "    args.valid_frac = 0.0\n",
    "    args.n_layer_e3gnn = 5\n",
    "    args.n_hidden_e3nn = 256\n",
    "    args.msg_cutoff_e3nn = 12.0\n",
    "    args.n_hidden_xformer = 256\n",
    "    args.n_embd_common = 256\n",
    "    args.n_layer_xformer = 16\n",
    "    args.n_head = 16\n",
    "    args.max_n_seq = 250  # max the model can forward\n",
    "    #    args.n_seq = 90 # max allowed in training.\n",
    "    args.n_seq = 80  # max allowed in training.\n",
    "    args.biases = True\n",
    "    args.torch_emb = False\n",
    "    args.norm_clips = True\n",
    "    args.norm_embed = False\n",
    "    args.token_mlp = True\n",
    "\n",
    "    args.tokenizer_vocab = \"mar\"\n",
    "    args.p_dataset = 0.2\n",
    "    args.p_formula = 0.0\n",
    "    args.p_fim = 0.0\n",
    "    args.p_graph = 0.0\n",
    "    args.p_clip = 0.9\n",
    "    args.p_clip_emb_smi = 0.5\n",
    "    args.p_randsmiles = 0.3\n",
    "    args.batch_size = 160\n",
    "\n",
    "    args.online = False  # Possible offline training of an end-to-end clip\n",
    "    args.lr = 5.0e-4\n",
    "    args.weight_decay = 0.1\n",
    "\n",
    "    args.dtype = \"float\"\n",
    "    args.n_epochs = 25\n",
    "    args.clip_grad = 10\n",
    "    args.test_interval = 2\n",
    "    args.debug = False\n",
    "\n",
    "    args.resume_optimizer = False\n",
    "    # resume from checkpoint file\n",
    "    # args.resume_document = ''\n",
    "\n",
    "    args.ngrad_to_save = 2e6\n",
    "\n",
    "    # output logs\n",
    "    args.output_dir = \"./logs/\"\n",
    "    # where to save model checkpoints\n",
    "    args.model_dir = \"./model_ckpts/\"\n",
    "    # where to save dataset cache\n",
    "    args.data_dir = \"./\"\n",
    "    args.model_filename = \"coati_grande\"\n",
    "\n",
    "    COATI_dataset(cache_dir=args.data_dir).get_data_pipe()\n",
    "    print(f\"running on {args.gpus} gpus\")\n",
    "    #########################################################\n",
    "    args.world_size = args.gpus * args.nodes\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"8899\"\n",
    "    mp.spawn(train_autoencoder, nprocs=args.gpus, args=(args,))\n",
    "    #########################################################\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
