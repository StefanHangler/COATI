{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (0.20.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "%pip install polars\n",
    "# Manually specify the path\n",
    "parent_dir = '/Users/stefanhangler/Documents/Uni/Msc_AI/3_Semester/Seminar_Practical Work/Code.nosync/COATI'\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import os\n",
    "import pickle # Stefan\n",
    "from torch.utils.data.datapipes.iter import IterableWrapper # Stefan\n",
    "\n",
    "from coati.common.util import dir_or_file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COATI_dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_dir,\n",
    "        fields=[\"smiles\", \"atoms\", \"coords\"],\n",
    "        test_split_mode=\"row\",\n",
    "        test_frac=0.02,  # in percent.\n",
    "        valid_frac=0.02,  # in percent.\n",
    "    ):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.summary = {\"dataset_type\": \"coati\", \"fields\": fields}\n",
    "        self.test_frac = test_frac\n",
    "        self.fields = fields\n",
    "        self.valid_frac = valid_frac\n",
    "        assert int(test_frac * 100) >= 0 and int(test_frac * 100) <= 50\n",
    "        assert int(valid_frac * 100) >= 0 and int(valid_frac * 100) <= 50\n",
    "        assert int(valid_frac * 100 + test_frac * 100) < 50\n",
    "        self.test_split_mode = test_split_mode\n",
    "\n",
    "    def partition_routine(self, row):\n",
    "        \"\"\" \"\"\"\n",
    "        # if not \"mod_molecule\" in row:\n",
    "        #     tore = [\"raw\"]\n",
    "        #     tore.append(\"train\")\n",
    "        #     return tore\n",
    "        # else:\n",
    "        #     tore = [\"raw\"]\n",
    "\n",
    "        #     if row[\"mod_molecule\"] % 100 >= int(\n",
    "        #         (self.test_frac + self.valid_frac) * 100\n",
    "        #     ):\n",
    "        #         tore.append(\"train\")\n",
    "        #     elif row[\"mod_molecule\"] % 100 >= int((self.test_frac * 100)):\n",
    "        #         tore.append(\"valid\")\n",
    "        #     else:\n",
    "        #         tore.append(\"test\")\n",
    "\n",
    "        #     return tore\n",
    "        # Stefan --->\n",
    "        partition = row.get('partition', 'raw')\n",
    "        return [partition]\n",
    "\n",
    "    def get_data_pipe(\n",
    "        self,\n",
    "        rebuild=False,\n",
    "        batch_size=32,\n",
    "        partition: str = \"raw\",\n",
    "        required_fields=[],\n",
    "        distributed_rankmod_total=None,\n",
    "        distributed_rankmod_rank=1,\n",
    "        xform_routine=lambda X: X,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Look for the cache locally\n",
    "        then on s3 if it's not available locally\n",
    "        then return a pipe to the data.\n",
    "        \"\"\"\n",
    "        # print(f\"trying to open a {partition} datapipe for...\")\n",
    "        # if (\n",
    "        #     not dir_or_file_exists(os.path.join(self.cache_dir, S3_PATH, \"0.pkl\"))\n",
    "        # ) or rebuild:\n",
    "        #     makedir(self.cache_dir)\n",
    "        #     query_yes_no(\n",
    "        #         f\"Will download ~340 GB of data to {self.cache_dir} . This will take a while. Are you sure?\"\n",
    "        #     )\n",
    "        #     copy_bucket_dir_from_s3(S3_PATH, self.cache_dir)\n",
    "\n",
    "        # pipe = (\n",
    "        #     FileLister(\n",
    "        #         root=os.path.join(self.cache_dir, S3_PATH),\n",
    "        #         recursive=False,\n",
    "        #         masks=[\"*.pkl\"],\n",
    "        #     )\n",
    "        #     .shuffle()\n",
    "        #     .open_files(mode=\"rb\")\n",
    "        #     .unstack_pickles()\n",
    "        #     .unbatch()\n",
    "        #     .shuffle(buffer_size=200000)\n",
    "        # )\n",
    "        # pipe = pipe.ur_batcher(\n",
    "        #     batch_size=batch_size,\n",
    "        #     partition=partition,\n",
    "        #     xform_routine=xform_routine,\n",
    "        #     partition_routine=self.partition_routine,\n",
    "        #     distributed_rankmod_total=distributed_rankmod_total,\n",
    "        #     distributed_rankmod_rank=distributed_rankmod_rank,\n",
    "        #     direct_mode=False,\n",
    "        #     required_fields=self.fields,\n",
    "        # )\n",
    "        # return pipe\n",
    "\n",
    "        print(f\"trying to open a {partition} datapipe for...\")\n",
    "\n",
    "        # Path to your preprocessed pickle file\n",
    "        pickle_file = os.path.join(self.cache_dir, \"train_valid_test_guacamol.pkl\")\n",
    "\n",
    "        if not dir_or_file_exists(pickle_file):\n",
    "            raise FileNotFoundError(f\"{pickle_file} does not exist. Please ensure the file is in the correct location.\")\n",
    "\n",
    "        # Load the pickle file\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # Filter data based on the partition\n",
    "        filtered_data = [row for row in data if row.get('partition') == partition]\n",
    "\n",
    "        # Create an IterableWrapper from the filtered data\n",
    "        pipe = IterableWrapper(filtered_data)\n",
    "\n",
    "        # Shuffle, batch, and transform the data\n",
    "        pipe = pipe.shuffle().batch(batch_size).map(xform_routine)\n",
    "\n",
    "        return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (1.34.122)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.122 in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (from boto3) (1.34.122)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.122->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.122->boto3) (1.26.12)\n",
      "Requirement already satisfied: six>=1.5 in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.122->boto3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: rdkit in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (2023.9.6)\n",
      "Requirement already satisfied: numpy in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (from rdkit) (1.23.3)\n",
      "Requirement already satisfied: Pillow in /Users/stefanhangler/opt/anaconda3/envs/DLNN1/lib/python3.10/site-packages (from rdkit) (9.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "%pip install boto3\n",
    "%pip install rdkit\n",
    "from coati.training.train_coati import train_autoencoder, do_args\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "from coati.data.dataset import COATI_dataset\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning... unparsed:  ['--f=/Users/stefanhangler/Library/Jupyter/runtime/kernel-v2-15315ImwoiFF0kDg.json']\n",
      "new shuffle csv pipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UrBatcher"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = do_args()\n",
    "args.nodes = 1  # total num nodes.\n",
    "args.nr = 0  # rank of this node.\n",
    "# note args.gpus will default to the # gpus on this node.\n",
    "args.data_parallel = True\n",
    "\n",
    "args.test_frac = 0.02\n",
    "args.valid_frac = 0.0\n",
    "args.n_layer_e3gnn = 5\n",
    "args.n_hidden_e3nn = 256\n",
    "args.msg_cutoff_e3nn = 12.0\n",
    "args.n_hidden_xformer = 256\n",
    "args.n_embd_common = 256\n",
    "args.n_layer_xformer = 16\n",
    "args.n_head = 16\n",
    "args.max_n_seq = 250  # max the model can forward\n",
    "#    args.n_seq = 90 # max allowed in training.\n",
    "args.n_seq = 80  # max allowed in training.\n",
    "args.biases = True\n",
    "args.torch_emb = False\n",
    "args.norm_clips = True\n",
    "args.norm_embed = False\n",
    "args.token_mlp = True\n",
    "\n",
    "args.tokenizer_vocab = \"mar\"\n",
    "args.p_dataset = 0.2\n",
    "args.p_formula = 0.0\n",
    "args.p_fim = 0.0\n",
    "args.p_graph = 0.0\n",
    "args.p_clip = 0.9\n",
    "args.p_clip_emb_smi = 0.5\n",
    "args.p_randsmiles = 0.3\n",
    "args.batch_size = 160\n",
    "\n",
    "args.online = False  # Possible offline training of an end-to-end clip\n",
    "args.lr = 5.0e-4\n",
    "args.weight_decay = 0.1\n",
    "\n",
    "args.dtype = \"float\"\n",
    "args.n_epochs = 25\n",
    "args.clip_grad = 10\n",
    "args.test_interval = 2\n",
    "args.debug = False\n",
    "\n",
    "args.resume_optimizer = False\n",
    "# resume from checkpoint file\n",
    "# args.resume_document = ''\n",
    "\n",
    "args.ngrad_to_save = 2e6\n",
    "\n",
    "# output logs\n",
    "args.output_dir = \"./logs/\"\n",
    "# where to save model checkpoints\n",
    "args.model_dir = \"./model_ckpts/\"\n",
    "# where to save dataset cache\n",
    "args.data_dir = \"./\"\n",
    "args.model_filename = \"coati_grande\"\n",
    "\n",
    "COATI_dataset(cache_dir=args.data_dir).get_data_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLNN1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
